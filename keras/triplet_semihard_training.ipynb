{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing as mp\n",
    "import math\n",
    "import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "import json\n",
    "import PIL\n",
    "from functools import partial, update_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_size = 299     # input image size for network\n",
    "margin = 0.3       # margin for triplet loss\n",
    "batch_size = 42    # size of mini-batch\n",
    "num_triplet = 700 \n",
    "valid_frequency = 100\n",
    "num_epoch = 3600 \n",
    "log_dir = './logs/triplet_semihard_v3'\n",
    "checkpoint_dir = 'checkpoints/'\n",
    "recall_log_file = './logs/recall_triplet_semihard_v3.json'\n",
    "recall_values = [1, 3, 5, 10, 25, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# utility function to freeze some portion of a function's arguments\n",
    "def wrapped_partial(func, *args, **kwargs):\n",
    "    partial_func = partial(func, *args, **kwargs)\n",
    "    update_wrapper(partial_func, func)\n",
    "    return partial_func\n",
    "\n",
    "# calculate recall score\n",
    "def recall(y_true, y_pred):\n",
    "    return min(len(set(y_true) & set(y_pred)), 1)\n",
    "\n",
    "# margin triplet loss\n",
    "def margin_triplet_loss(y_true, y_pred, margin, batch_size):\n",
    "    out_a = tf.gather(y_pred, tf.range(0, batch_size, 3))\n",
    "    out_p = tf.gather(y_pred, tf.range(1, batch_size, 3))\n",
    "    out_n = tf.gather(y_pred, tf.range(2, batch_size, 3))\n",
    "    \n",
    "    loss = K.maximum(margin\n",
    "                 + K.sum(K.square(out_a-out_p), axis=1)\n",
    "                 - K.sum(K.square(out_a-out_n), axis=1),\n",
    "                 0.0)\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    no_top_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "    x = no_top_model.output\n",
    "    x = Dense(512, activation='elu', name='fc1')(x)\n",
    "    x = Dense(128, name='fc2')(x)\n",
    "    x = Lambda(lambda x: K.l2_normalize(x, axis=1), name='l2_norm')(x)\n",
    "    return Model(no_top_model.inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=wrapped_partial(margin_triplet_loss, margin=margin, batch_size=batch_size), optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used mongodb to store dataset. Document structure:\n",
    "- type: define split on train, validation and test sets, possible values - 'train', 'val', 'test'\n",
    "- seller_img: list of IDs of seller's images\n",
    "- user_img: list of IDs of user's images\n",
    "\n",
    "Example:\n",
    "```javascript\n",
    "{\n",
    "    'type': 'test',\n",
    "    'seller_img': ['HTB1s7ZiLFXXXXatXFXXq6xXFXXXu', \n",
    "                'HTB1pGaAKXXXXXczXXXXq6xXFXXXN'],\n",
    "    'user_img': ['UTB8KtbUXXPJXKJkSahVq6xyzFXaE',\n",
    "                'UTB8OeDUXgnJXKJkSaelq6xUzXXag',\n",
    "                'UTB8h7HUXnzIXKJkSafVq6yWgXXap',\n",
    "                'UTB8auL6XevJXKJkSajhq6A7aFXaa',\n",
    "                'UTB8rrevXevJXKJkSajhq6A7aFXa5',\n",
    "                'UTB8WUCuXXPJXKJkSahVq6xyzFXa6',\n",
    "                'UTB8MHmvXXfJXKJkSamHq6zLyVXa1']\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(connect=False)\n",
    "db = client['aliexpress']\n",
    "coll = db['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_seller_images = []\n",
    "train_user_images = []\n",
    "for item in coll.find({'type': 'train'}):\n",
    "    train_seller_images.append(item['seller_img'])\n",
    "    train_user_images.append(item['user_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train_seller = mlb.fit_transform(train_seller_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_images_seller = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train_user = mlb.fit_transform(train_user_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_images_user = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_images_by_class = np.asarray(X_train_user.sum(axis=1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val_images = [(x['seller_img'], x['user_img']) for x in coll.find({'type': 'val'})]\n",
    "val_image_to_class = {}\n",
    "for i, item in enumerate(val_images):\n",
    "    for x in item[0] + item[1]:\n",
    "        val_image_to_class[x] = i\n",
    "\n",
    "val_images_clean = [(x['seller_img_clean'], x['user_img_clean']) for x in coll.find({'type': 'val', 'clean': True})]\n",
    "val_image_clean_to_class = {}\n",
    "for i, item in enumerate(val_images_clean):\n",
    "    for x in item[0] + item[1]:\n",
    "        val_image_clean_to_class[x] = i\n",
    "\n",
    "test_images = [(x['seller_img'], x['user_img']) for x in coll.find({'type': 'test'})]\n",
    "\n",
    "seller_images = [x for item in coll.find({}) for x in item['seller_img']]\n",
    "val_user_images = [x for item in val_images for x in item[1]]\n",
    "val_user_clean_images = [x for item in val_images_clean for x in item[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load image by id without augmentations\n",
    "def preprocess_image_worker(media_id):\n",
    "    img = Image.open(('./images/'+media_id+'.jpg')).convert('RGB')\n",
    "    img = img.resize((img_size, img_size))\n",
    "\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load image by id with augmentations\n",
    "def preprocess_image_worker_aug(media_id):\n",
    "    img = Image.open(('./images/'+media_id+'.jpg')).convert('RGB')\n",
    "    img = img.crop((int(np.random.uniform(0, 0.05)*img.width), int(np.random.uniform(0, 0.05)*img.height),\n",
    "                  int(np.random.uniform(0.95, 1.)*img.width), int(np.random.uniform(0.95, 1.)*img.height)))\n",
    "    if np.random.randint(2) == 0:\n",
    "        img = img.transpose(np.random.choice([PIL.Image.FLIP_LEFT_RIGHT, PIL.Image.FLIP_TOP_BOTTOM, PIL.Image.ROTATE_90, PIL.Image.ROTATE_180, PIL.Image.ROTATE_270, PIL.Image.TRANSPOSE]))\n",
    "    img = img.resize((img_size, img_size))\n",
    "\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data generator, divides list images into mini-batches\n",
    "def batch_generator_predict(pool, batch_size, images):\n",
    "    i = 0\n",
    "    while True:\n",
    "        batch = images[i:i+batch_size]\n",
    "        i += batch_size\n",
    "        if len(batch) == 0:\n",
    "            yield np.zeros((0, img_size, img_size, 3))\n",
    "        else:\n",
    "            result = pool.map(preprocess_image_worker, batch)\n",
    "            X_batch = np.concatenate(result, axis=0)\n",
    "            yield X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-hard negative mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SemiHardNegativeSampler:\n",
    "    def __init__(self, pool, batch_size, num_samples):\n",
    "        self.pool = pool\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = num_samples\n",
    "        self.resample()\n",
    "        \n",
    "    # sample triplets with semi-hard negatives\n",
    "    def resample(self):        \n",
    "        sample_classes = np.random.choice(np.arange(X_train_user.shape[0]), p=num_images_by_class/num_images_by_class.sum(), size=self.num_samples)\n",
    "\n",
    "        sample_images = []\n",
    "        for i in sample_classes:\n",
    "            sample_images.append(train_images_user[np.random.choice(X_train_user[i].nonzero()[1], replace=False)])\n",
    "            sample_images.append(train_images_seller[np.random.choice(X_train_seller[i].nonzero()[1], replace=False)])\n",
    "        sample_images = np.array(sample_images)\n",
    "\n",
    "        pred_sample = model.predict_generator(batch_generator_predict(pool, 32, sample_images), math.ceil(len(sample_images)/32), max_q_size=1, workers=1)\n",
    "\n",
    "        a = pred_sample[np.arange(0, len(pred_sample), 2)]\n",
    "        p = pred_sample[np.arange(1, len(pred_sample), 2)]\n",
    "        triplets = []\n",
    "        self.dists = []\n",
    "\n",
    "        for i in range(self.num_samples):\n",
    "            d = np.square(a[i] - p[i]).sum()\n",
    "            neg_sample_classes = (sample_classes != sample_classes[i]).nonzero()[0]\n",
    "\n",
    "            neg = p[neg_sample_classes]\n",
    "\n",
    "            neg_ids = sample_images.reshape((-1, 2))[neg_sample_classes, 1]\n",
    "\n",
    "            d_neg = np.square(neg - a[i]).sum(axis=1)\n",
    "\n",
    "            semihard = np.where(d_neg > d)[0]\n",
    "\n",
    "            if len(semihard) == 0:\n",
    "                n = np.argmax(d_neg)\n",
    "            else:\n",
    "                n = semihard[np.argmin(d_neg[semihard])]\n",
    "                \n",
    "            self.dists.append(d_neg[n]-d)\n",
    "\n",
    "            triplets.append(np.concatenate([sample_images.reshape((-1, 2))[i], np.array([neg_ids[n]])]))\n",
    "\n",
    "        self.triplets = np.array(triplets)\n",
    "        \n",
    "    # data generator for triplets\n",
    "    def batch_generator(self):\n",
    "        i = 0\n",
    "        while True:\n",
    "            batch = self.triplets[i:i+self.batch_size//3].ravel()\n",
    "            \n",
    "            i += self.batch_size//3\n",
    "            if len(batch) == 0:\n",
    "                yield np.zeros((0, img_size, img_size, 3))\n",
    "            else:\n",
    "                result = pool.map(preprocess_image_worker_aug, batch)\n",
    "                X_batch = np.concatenate(result, axis=0)\n",
    "                yield X_batch, np.zeros(len(batch))\n",
    "\n",
    "    # return data generator for triplets\n",
    "    def get_generator(self):\n",
    "        gen = self.batch_generator()\n",
    "        return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sampler = SemiHardNegativeSampler(pool, batch_size, num_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pool of processes for parallel data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pool = mp.Pool(processes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach tensorboard to monitor learining process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tensorboard = keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "                 histogram_freq=1, \n",
    "                 write_graph=False, \n",
    "                 write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process\n",
    "Each epoch we \n",
    "- train model on triplets with semi-hard negatives from `sampler`\n",
    "- resample triplets\n",
    "- do validation and save model with frequency `valid_frequency`\n",
    "\n",
    "I use annoy index for nearest neighbors search to speedup validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_epoch):\n",
    "    train_gen = sampler.get_generator()\n",
    "    h = model.fit_generator(train_gen, steps_per_epoch=num_triplets//(batch_size//3), epochs=epoch+1, initial_epoch=epoch, verbose=2,\n",
    "                        max_q_size=1, callbacks=[tensorboard,])\n",
    "    sampler.resample()\n",
    "    \n",
    "    if epoch%valid_frequency == 0 and epoch != 0:\n",
    "        seller_pred = model.predict_generator(batch_generator_predict(pool, 32, seller_images), math.ceil(len(seller_images)/32), max_q_size=1, workers=1)\n",
    "        val_user_pred = model.predict_generator(batch_generator_predict(pool, 32, val_user_images), math.ceil(len(val_user_images)/32), max_q_size=1, workers=1)\n",
    "        val_user_clean_pred = model.predict_generator(batch_generator_predict(pool, 32, val_user_clean_images), math.ceil(len(val_user_clean_images)/32), max_q_size=1, workers=1)\n",
    "\n",
    "        search_index = AnnoyIndex(128, metric='euclidean')\n",
    "        for i in range(len(seller_pred)):\n",
    "            search_index.add_item(i, seller_pred[i])\n",
    "        search_index.build(50)\n",
    "\n",
    "        recall_scores = {i: [] for i in recall_values}\n",
    "        for i in range(len(val_user_pred)):\n",
    "            r = search_index.get_nns_by_vector(val_user_pred[i], 100)\n",
    "            val_cl = val_image_to_class[val_user_images[i]]\n",
    "            for k in recall_values:\n",
    "                recall_scores[k].append(recall(val_images[val_cl][0], [seller_images[i] for i in r[:k]]))\n",
    "\n",
    "        print ('val on full')\n",
    "        for k in recall_values:\n",
    "            print (k, np.mean(recall_scores[k]))\n",
    "\n",
    "        val_recall = [(k, np.mean(recall_scores[k])) for k in recall_values]\n",
    "\n",
    "        recall_scores = {i: [] for i in recall_values}\n",
    "\n",
    "        for i in range(len(val_user_clean_pred)):\n",
    "            r = search_index.get_nns_by_vector(val_user_clean_pred[i], 100)\n",
    "            val_cl = val_image_clean_to_class[val_user_clean_images[i]]\n",
    "            for k in recall_values:\n",
    "                recall_scores[k].append(recall(val_images_clean[val_cl][0], [seller_images[i] for i in r[:k]]))\n",
    "\n",
    "        print ('val on clean')\n",
    "        for k in recall_values:\n",
    "            print (k, np.mean(recall_scores[k]))\n",
    "\n",
    "        val_recall_clean = [(k, np.mean(recall_scores[k])) for k in recall_values]\n",
    "        \n",
    "        try:\n",
    "            with open(recall_log_file, 'r') as f:\n",
    "                recall_log = json.load(f)\n",
    "        except:\n",
    "            recall_log = []\n",
    "\n",
    "        recall_log.append((epoch, val_recall, val_recall_clean))\n",
    "\n",
    "        with open(recall_log_file, 'w') as f:\n",
    "            json.dump(recall_log, f)\n",
    "            \n",
    "        model.save_weights(os.path.join(checkpoint_dir, 'triplet_semihard_%d.keras'%epoch))\n",
    "        \n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
